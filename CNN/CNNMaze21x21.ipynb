{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "import math\n",
    "from datasets import Dataset, Features, Array2D, Value, load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 3)\n",
      "{'data': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
      "        [1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
      "        [1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
      "        [1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1],\n",
      "        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1],\n",
      "        [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1],\n",
      "        [1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1],\n",
      "        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'filename': '21HnKgridsize_1_step0_DOWN', 'label': tensor(4)}\n"
     ]
    }
   ],
   "source": [
    "loaded_data = load_from_disk(\n",
    "        \"/Users/yijiachen/Desktop/50.021 Artificial Intelligence/Project/code/code-repo/dataset/even_label/21x21_train.hf\"\n",
    "    ).with_format(\"torch\")\n",
    "\n",
    "print(loaded_data.shape)\n",
    "print(loaded_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_set_size = math.floor(0.8*loaded_data.shape[0])\n",
    "train_set, val_set = torch.utils.data.random_split(loaded_data, [train_set_size, loaded_data.shape[0] - train_set_size])\n",
    "trainLoader = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "testLoader = torch.utils.data.DataLoader(val_set, batch_size=16, shuffle=False)\n",
    "\n",
    "classes = ('END', 'LEFT', 'RIGHT', 'UP', 'DOWN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1), tensor(4), tensor(2), tensor(3))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiUAAACaCAYAAAB2bjhiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAASH0lEQVR4nO3dbYhU5f/H8c9o7qpls5o46+YMbiBZSQqbN6sVFQtmEWk+yAeBFCiuu5IZhEa5KdFaEYpmPdLV6EYK0gdGBq6pGWuxCxLeW0gu6ExK7Ox6N2vu9XvQ3/k7OtrOzrm5zsz7BQfcM2fOuWbOZ49fzn7nmpAxxggAAMBn/fweAAAAgERRAgAALEFRAgAArEBRAgAArEBRAgAArEBRAgAArEBRAgAArEBRAgAArEBRAgAArEBRAgAArHCHWztev369PvzwQ8XjcY0fP17r1q3TpEmT/vN5PT09On36tIYMGaJQKOTW8OAQY4y6urpUUVGhfv0ya9y+ZkAiB0HjRg7IQLBwLcDtMpDLThy3ZcsWU1JSYjZu3GgOHTpk5s2bZ8rKykwikfjP57a3txtJLAFb2tvbHcsAOQju4mQOyEAwF64FLDdmIBchY5z/Qr7Jkydr4sSJ+vjjjyX9W+lGo1EtWrRIS5cuve1zk8mkysrK9Nprr6m0tNTpocFhqVRKq1evVkdHh8LhcHp9PhmQyEHQuJEDMhAsXAtwqwzkwvE/33R3d6utrU3Lli1Lr+vXr59qamrU0tJy0/apVEqpVCr9c1dXlySptLSUAAbI9bdVc82ARA4KRT45IAOFgWsB8vkzm+ONrufOndPVq1cViUQy1kciEcXj8Zu2b2xsVDgcTi/RaNTpIcFjuWZAIgeFiGsBuBYgV75/+mbZsmVKJpPppb293e8hwQfkAGQAEjkodo7/+Wb48OHq37+/EolExvpEIqHy8vKbtu/tLbl33nnHqSEGXrb3wqv3pzfHyTUDUuHn4Fbj9uL1uJEXN3KQTwZyeY35PN/pdbdb75d8X8/1iulaUGg56C2nx+34nZKSkhJVVVWpubk5va6np0fNzc2qrq52+nCwEBmARA5ABpA7V+YpWbJkiebOnatHHnlEkyZN0po1a3ThwgW9/PLLbhwOFiIDkMgByABy40pR8uKLL+rs2bNavny54vG4JkyYoB07dtzU7ITCRQYgkQOQAeTGtRld6+vrVV9f79buEQBkABI5ABlA77lWlDgtCE1AfjagZuNG85+NgjLO3nKy0dCtY8M5Tr+/fubHbzY1/AfpOL3lxXh8/0gwAACARFECAAAsQVECAACsQFECAACsQKOrT/J5PYX2XuQrl/fDr4Y/r5pSi1W+GfDq2G48H7cX5PPT22MXUiMzd0oAAIAVKEoAAIAVKEoAAIAVKEoAAIAVKEoAAIAVAvPpm2zc6Kq2rdM6qB3UbsmnG92Jbf3Yn1v7DKp8M+DXtO65PD+f47jxO1KIbLveenHegnDOuVMCAACsQFECAACsQFECAACsQFECAACsUDSNroXUHGTbeLzk1ZTRQW2CC8px8hHURuKgNF8HIQNeCkIjvBvHdrqxure4UwIAAKxAUQIAAKxAUQIAAKxAUQIAAKwQmEZXP2cpDEpjkh/7s4FXza9uHDsfQR23G7yaVTUf+c4wy/num6A0GDstqDP7cqcEAABYgaIEAABYgaIEAABYgaIEAABYoeAaXUOhUNb1DQ0Njh7Hthk/swnCGJ0QlIZGv45TiOc8H07/jvvZXE8GMrkxY6lf+3Njn7ZdK7PhTgkAALACRQkAALACRQkAALACRQkAALBCYBpds8nWiNPbhtZbPT+f7Zx+rpfHCXIjXBAaeov5/DgtCO+lGw2FQbgOecmrJl+vmk39arK1rfmVOyUAAMAKFCUAAMAKFCUAAMAKFCUAAMAKBdfo6tU+/Wy2C/LrDoogNCgHocHXDV797nl1bry4Fvh5zXCL38cvFLa9j9wpAQAAVqAoAQAAVqAoAQAAVsi5KNm7d6+ee+45VVRUKBQKadu2bRmPG2O0fPlyjRw5UoMGDVJNTY1OnDjh1HhhgT///FNfffWVPvroI61ateqmx8lA4bs+AytWrNDx48czHicDxYFrAZyWc6PrhQsXNH78eL3yyit64YUXbnr8gw8+0Nq1a7V582ZVVlbq7bff1vTp03X48GENHDiwzwMttOZOv47txOx93d3dikQimjBhgr7++uubHncrA7cbk237zIefX1feW0HIgBuNpW7I5/U4vS7Xbf3MQTa2ndtc2DZ2vxrpcy5KZsyYoRkzZmR9zBijNWvW6K233tLzzz8vSfrss88UiUS0bds2zZkzJ7/RwgpjxozRmDFjsj5GBooDGYBEDuA8R3tKTp48qXg8rpqamvS6cDisyZMnq6WlJetzUqmUOjs7MxYEV18yIJGDQkIGIJED9I2jRUk8HpckRSKRjPWRSCT92I0aGxsVDofTSzQadXJI8FhfMiCRg0JCBiCRA/SN75++WbZsmZLJZHppb2/3e0jwATkAGYBEDoqdo0VJeXm5JCmRSGSsTyQS6cduVFpaqrvvvjtjQXD1JQMSOSgkZAASOUDfODrNfGVlpcrLy9Xc3KwJEyZIkjo7O/XLL7+otrY2r30HYdpvr45t85TwbmZAsu/TNzafi9txczy2ZCAUCmVdb4zJewzXC2oG3OZ2DnorKP93eDHOIGQw56Lk/Pnz+v3339M/nzx5UgcOHNCwYcMUi8W0ePFivfvuuxozZkz6I2AVFRWaOXOmk+OGj7q7u/X3339nrPvtt98Ui8XIQJG4MQMdHR2SpPb2dj300ENkoEhwLYDTci5KWltb9eSTT6Z/XrJkiSRp7ty52rRpk9544w1duHBB8+fPV0dHhx599FHt2LHDlc+kwx+nT5/W5s2bM9Y99thjZKCI3JiBXbt2SZLee+89ffHFF2SgSHAtgNNyLkqeeOKJ297+DIVCWrlypVauXJnXwGCv0aNHq6GhQdK/H99btWqVkslk+m+/ZKDwXZ8B6f9z8Omnn0oiA8WCawGc5vunbwAAACSHG129FoSmnVuhOc45XjW/FtI5c+LrBmySbYzX38n5r23zOU6+07rnc2z0XRC+zqGQxtNb3CkBAABWoCgBAABWoCgBAABWoCgBAABWCHSjazZuNJMFZUbA3uyvEJvt/GzazLf51Yu8FVpTa28F5bX4ec0pNEGYkTsox+ktp8fDnRIAAGAFihIAAGAFihIAAGAFihIAAGCFgmt0vZUgzN7np6A02/k5q2oQZnQNwhjz5WezdjG8v9nY+Hpsm7HUq/coFArdtO5Wsxc7jRldAQBA0aAoAQAAVqAoAQAAVqAoAQAAViiaRlen2dj4VQzybcgNQiNbb9k2Hq941eDoZ1Zolrb/+L3hRg6MMb3aLgjvTzbcKQEAAFagKAEAAFagKAEAAFagKAEAAFYouEbXYv36aGRyo/k1CM2HxcqrRj83ml/9alL0qkHcS0EYZy458OrYNuFOCQAAsAJFCQAAsAJFCQAAsAJFCQAAsEKgG11ta9Tys8k2yDNIuiEoTY5OHydfhZaDfAThPBTSDMVusa3BOBdB+H/L6TFypwQAAFiBogQAAFiBogQAAFiBogQAAFiBogQAAFghMJ++CcJU4Nl41dkc1PfHa0533Hv1SSinj7NixYqs6xsaGvq8T9v4+YknrzLghiB8KsVLxfrVJX7lgDslAADAChQlAADAChQlAADAChQlAADACgXX6OrW853enxfTQwdlavN82TalvBf7y2Wf2bYzxjg7GJ959f4G+fekGHh1zQtqDoIwbu6UAAAAK1CUAAAAK1CUAAAAK+TUU9LY2Khvv/1WR48e1aBBgzR16lS9//77uv/++9PbXL58Wa+//rq2bNmiVCql6dOn65NPPlEkEnF88PDHTz/9pKNHj+rcuXO6445/I3TixAlVVVWltyEHhe3GDNx77703bUMGChsZgBtCJoeOt6efflpz5szRxIkT9c8//+jNN9/UwYMHdfjwYd15552SpNraWn333XfatGmTwuGw6uvr1a9fP/3888+9OkZnZ6fC4bCWLl2q0tLSvr0quOrzzz/XuHHjVFFRocuXL6upqUnRaFRHjhwhB0Xi+gz09PRo586d+uOPP3T69GmNHDlSEhkodF5kQCIHQZJKpbRq1Solk0ndfffdfdpHTndKduzYkfHzpk2bNGLECLW1tenxxx9XMpnUhg0b9OWXX+qpp56SJDU1NemBBx7Q/v37NWXKlD4NEnZ56aWX0v9OpVKSpPb2dnJQRK7PgCQ9++yzWrt2rQ4cOKCRI0eSgSJABuCGvHpKksmkJGnYsGGSpLa2Nl25ckU1NTXpbcaOHatYLKaWlpas+0ilUurs7MxYEEzkoHhdK06HDh0qiQwUIycycG0/5KB49bko6enp0eLFizVt2jSNGzdOkhSPx1VSUqKysrKMbSORiOLxeNb9NDY2KhwOp5doNNrXIcEH1/76N2XKFHJQpIwx2rlzpyTpwQcflEQGio1TGZDIQbHr8+RpdXV1OnjwoPbt25fXAJYtW6YlS5akf04mk4rFYumqG3b7/vvvJUkbNmzIaz/kILh++OEHnT17VlJ+k7KRgeByKgMSOQiya+conwz0qSipr6/X9u3btXfvXo0aNSq9vry8XN3d3ero6MiojhOJhMrLy7Puq7S0NKN56dqtutWrV/dlaPDJ9U1N5KB4dXV1KRwOk4Eilk8GJHJQCK5loC9yKkqMMVq0aJG2bt2q3bt3q7KyMuPxqqoqDRgwQM3NzZo9e7Yk6dixYzp16pSqq6t7dYyKigq1t7dryJAh6urqUjQaVXt7e587eeE8Y4xeffVVbd68WXv27NF9992nioqK9ONO5sAYo1gsRgYsc30G9u7dq4cfflhdXV3pHHAtKHxeZEDiWhAEnZ2dikajOnXqlEKhUMb/BzkzOaitrTXhcNjs3r3bnDlzJr1cvHgxvc2CBQtMLBYzu3btMq2traa6utpUV1fncpi0ZDJpJJlkMtmn58Md13IgyRw/ftzVHJABO3mZAWPIgY3IAK5x8tzkVJRIyro0NTWlt7l06ZJZuHChGTp0qBk8eLCZNWuWOXPmTJ8GRwjt5GUOyICduBaADOAaJ89NTpOnee3apDn5TMQCd3h1bsiAvbw8N+TATmQAkrPnxurvviktLVVDQwOz+FnIq3NDBuzl5bkhB3YiA5CcPTdW3ykBAADFw+o7JQAAoHhQlAAAACtQlAAAACtQlAAAACtYW5SsX79eo0eP1sCBAzV58mT9+uuvfg+p6DQ2NmrixIkaMmSIRowYoZkzZ+rYsWMZ21y+fFl1dXW65557dNddd2n27NlKJBKOjYEc+IsMQCIH8DADec904oItW7aYkpISs3HjRnPo0CEzb948U1ZWZhKJhN9DKyrTp083TU1N5uDBg+bAgQPmmWeeMbFYzJw/fz69zYIFC0w0GjXNzc2mtbXVTJkyxUydOtWR45MD/5EBGEMO4F0GrCxKJk2aZOrq6tI/X7161VRUVJjGxkYfR4W//vrLSDJ79uwxxhjT0dFhBgwYYL755pv0NkeOHDGSTEtLS97HIwf2IQMwhhzAvQxY9+eb7u5utbW1qaamJr2uX79+qqmpUUtLi48jQzKZlCQNGzZMktTW1qYrV65knKuxY8cqFovlfa7IgZ3IACRyAPcyYF1Rcu7cOV29elWRSCRjfSQSUTwe92lU6Onp0eLFizVt2jSNGzdOkhSPx1VSUpLxteSSM+eKHNiHDEAiB3A3A3c4OVAUrrq6Oh08eFD79u3zeyjwCRmARA7gbgasu1MyfPhw9e/f/6aO3UQiofLycp9GVdzq6+u1fft2/fjjjxo1alR6fXl5ubq7u9XR0ZGxvRPnihzYhQxAIgdwPwPWFSUlJSWqqqpSc3Nzel1PT4+am5tVXV3t48iKjzFG9fX12rp1q3bt2qXKysqMx6uqqjRgwICMc3Xs2DGdOnUq73NFDuxABiCRA3iYAUfbcR2yZcsWU1paajZt2mQOHz5s5s+fb8rKykw8Hvd7aEWltrbWhMNhs3v3bnPmzJn0cvHixfQ2CxYsMLFYzOzatcu0traa6upqU11d7cjxyYH/yACMIQfwLgNWFiXGGLNu3ToTi8VMSUmJmTRpktm/f7/fQyo6krIuTU1N6W0uXbpkFi5caIYOHWoGDx5sZs2aZc6cOePYGMiBv8gAjCEH8C4Dof87GAAAgK+s6ykBAADFiaIEAABYgaIEAABYgaIEAABYgaIEAABYgaIEAABYgaIEAABYgaIEAABYgaIEAABYgaIEAABYgaIEAABYgaIEAABY4X8F05gfcwO2rwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test the dataloader\n",
    "dataiter = iter(trainLoader)\n",
    "this_iter = next(dataiter)\n",
    "this_iter['data'], this_iter['label']\n",
    "# mazes, filenames, labels = next(dataiter)\n",
    "# mazes\n",
    "# this_iter\n",
    "\n",
    "fig, axes = plt.subplots(1,4)\n",
    "axes[0].imshow(this_iter['data'][0].numpy(), cmap='binary')\n",
    "axes[1].imshow(this_iter['data'][1].numpy(), cmap='binary')\n",
    "axes[2].imshow(this_iter['data'][2].numpy(), cmap='binary')\n",
    "axes[3].imshow(this_iter['data'][3].numpy(), cmap='binary')\n",
    "\n",
    "this_iter['label'][0], this_iter['label'][1], this_iter['label'][2], this_iter['label'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(6, 6), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=16, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=5, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 16, 16]             222\n",
      "         MaxPool2d-2              [-1, 6, 8, 8]               0\n",
      "            Conv2d-3             [-1, 16, 2, 2]           4,720\n",
      "         MaxPool2d-4             [-1, 16, 1, 1]               0\n",
      "            Linear-5                  [-1, 120]           2,040\n",
      "            Linear-6                   [-1, 84]          10,164\n",
      "            Linear-7                    [-1, 5]             425\n",
      "================================================================\n",
      "Total params: 17,571\n",
      "Trainable params: 17,571\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 0.07\n",
      "Estimated Total Size (MB): 0.09\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 6)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 7)\n",
    "        \n",
    "        self.fc1 = nn.Linear(16 * 1 * 1, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "n = 21\n",
    "summary(net, (1,n,n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define a Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.610\n",
      "[1,  4000] loss: 1.609\n",
      "[1,  6000] loss: 1.608\n",
      "[1,  8000] loss: 1.393\n",
      "[1, 10000] loss: 1.124\n",
      "[1, 12000] loss: 1.108\n",
      "[1, 14000] loss: 1.110\n",
      "[2,  2000] loss: 1.103\n",
      "[2,  4000] loss: 1.089\n",
      "[2,  6000] loss: 1.072\n",
      "[2,  8000] loss: 1.062\n",
      "[2, 10000] loss: 1.051\n",
      "[2, 12000] loss: 1.038\n",
      "[2, 14000] loss: 1.031\n",
      "[3,  2000] loss: 1.013\n",
      "[3,  4000] loss: 0.999\n",
      "[3,  6000] loss: 0.985\n",
      "[3,  8000] loss: 0.951\n",
      "[3, 10000] loss: 0.880\n",
      "[3, 12000] loss: 0.804\n",
      "[3, 14000] loss: 0.737\n",
      "[4,  2000] loss: 0.670\n",
      "[4,  4000] loss: 0.630\n",
      "[4,  6000] loss: 0.607\n",
      "[4,  8000] loss: 0.585\n",
      "[4, 10000] loss: 0.576\n",
      "[4, 12000] loss: 0.558\n",
      "[4, 14000] loss: 0.553\n",
      "[5,  2000] loss: 0.516\n",
      "[5,  4000] loss: 0.518\n",
      "[5,  6000] loss: 0.499\n",
      "[5,  8000] loss: 0.500\n",
      "[5, 10000] loss: 0.497\n",
      "[5, 12000] loss: 0.488\n",
      "[5, 14000] loss: 0.486\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "losslist = []\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, current_data in enumerate(trainLoader, 0):\n",
    "    # for i, data in enumerate(trainLoader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        # inputs, labels = data\n",
    "        # print(current_data)\n",
    "        inputs = current_data[\"data\"].float().reshape(-1, 1, n, n)\n",
    "        # print(inputs.shape)\n",
    "        labels = current_data[\"label\"]\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            losslist.append(running_loss / 2000)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './maze_net21x21_even_label.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the network on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3), tensor(4), tensor(4), tensor(4))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter_test = iter(testLoader)\n",
    "this_iter_test = next(dataiter_test)\n",
    "\n",
    "fig, axes = plt.subplots(1,4)\n",
    "axes[0].imshow(this_iter_test['data'][0].numpy(), cmap='binary')\n",
    "axes[1].imshow(this_iter_test['data'][1].numpy(), cmap='binary')\n",
    "axes[2].imshow(this_iter_test['data'][2].numpy(), cmap='binary')\n",
    "axes[3].imshow(this_iter_test['data'][3].numpy(), cmap='binary')\n",
    "\n",
    "this_iter_test['label'][0], this_iter_test['label'][1], this_iter_test['label'][2], this_iter_test['label'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-14.3480,   4.2604,   2.5923,   6.2273,   2.6549],\n",
       "        [ -3.4386,  -0.5880,   1.7068,   0.4667,   2.8640],\n",
       "        [-12.6588,  -0.1783,   0.8773,   5.9874,   7.4387],\n",
       "        [ -5.6570,   1.9482,   0.5027,  -0.4329,   5.5558],\n",
       "        [-16.2280,  11.9720,   8.4166,   3.0311,  -3.7781],\n",
       "        [ -4.9351,   2.7849,   4.9084,  -0.5860,  -0.0510],\n",
       "        [ -2.7555,  -0.6749,   1.5204,   0.0550,   2.8433],\n",
       "        [ 11.2700,  -5.7825,   2.3895,  -7.1826,   2.1024],\n",
       "        [-14.5590,   8.6247,   7.0591,   3.5674,  -1.9104],\n",
       "        [ -6.5542,  -4.7268,   8.9011,   5.4468,  -0.8207],\n",
       "        [-10.5179,   1.3684,   0.3887,   5.3889,   4.1844],\n",
       "        [-12.0069,  -0.1801,   1.3285,   6.7687,   5.2233],\n",
       "        [ -9.6028,   4.4515,   3.8209,   2.8351,  -0.1800],\n",
       "        [ -7.2088,   1.8725,   4.3279,   1.5841,   1.1502],\n",
       "        [ 11.9560,  -6.0305,   2.7031,  -7.4643,   1.7584],\n",
       "        [ -6.9812,   3.7639,   4.9783,   1.5338,  -1.7708]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = net(this_iter_test[\"data\"].float().reshape(-1,1,n,n))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  UP    DOWN  DOWN  DOWN \n"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at how the network performs on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test mazes: 78 %\n",
      "The F1 Score is: 0.7897524803174951\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "f1_true = None\n",
    "f1_predicted = None\n",
    "\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for current_data in testLoader:\n",
    "        mazes = current_data['data'].float().reshape(-1,1,n,n)\n",
    "        labels = current_data['label']\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(mazes)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        if f1_true is None:\n",
    "            f1_true = labels.detach().numpy()\n",
    "            f1_predicted = predicted.detach().numpy()\n",
    "        else:\n",
    "            f1_true = np.concatenate((f1_true, labels.detach().numpy()))\n",
    "            f1_predicted = np.concatenate((f1_predicted, predicted.detach().numpy()))\n",
    "\n",
    "print(f'Accuracy of the network on the test mazes: {100 * correct // total} %')\n",
    "\n",
    "# F1 score\n",
    "score = f1_score(f1_true, f1_predicted, average=\"macro\")\n",
    "print(f\"The F1 Score is: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm, what are the classes that performed well, and the classes that did not perform well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: END   is 100.0 %\n",
      "Accuracy for class: LEFT  is 71.7 %\n",
      "Accuracy for class: RIGHT is 75.3 %\n",
      "Accuracy for class: UP    is 66.7 %\n",
      "Accuracy for class: DOWN  is 81.3 %\n"
     ]
    }
   ],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for current_data in testLoader:\n",
    "        mazes = current_data['data'].float().reshape(-1,1,n,n)\n",
    "        labels = current_data['label']\n",
    "        outputs = net(mazes)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            # print(label, prediction)\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n",
    "\n",
    "# print(total_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mazerl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
